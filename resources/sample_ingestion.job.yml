resources:
  jobs:  # https://docs.databricks.com/aws/en/dev-tools/bundles/resources#job
    sample_ingestion:
      name: Sample Ingestion Job
      max_concurrent_runs: 1
      timeout_seconds: 0  # No timeout

      email_notifications:
        on_failure: ${var.failure_notification_emails}

      notification_settings:
        no_alert_for_skipped_runs: true
        no_alert_for_canceled_runs: true

      # Health rules for job monitoring
      # Docs: https://docs.databricks.com/aws/en/dev-tools/bundles/resources#job-health-rules
      health:
        rules:
          - metric: RUN_DURATION_SECONDS
            op: GREATER_THAN
            value: 3600  # Alert if job takes > 1 hour

      # OPTION 1: Serverless compute
      environments:
        - environment_key: default
          # https://docs.databricks.com/api/workspace/jobs/create#environments-spec
          spec:
            environment_version: "2"
            # Sample dependencies for serverless - add your packages here
            dependencies:
              - requests==2.32.3
              - databricks-sql-connector==4.0.0

      # OPTION 2: Custom cluster
      # Uncomment the section below and comment out the 'environments' section above
      # This provides better control over cluster specs, libraries, and configuration
      # Template source: templates/cluster_configs.yml (autoscale_cluster)
      # job_clusters:
      #   - job_cluster_key: job_cluster
      #     new_cluster:
      #       spark_version: 17.3.x-scala2.13
      #       node_type_id: Standard_DS3_v2  # AWS: i3.xlarge; Azure: Standard_DS3_v2; GCP: n1-highmem-4
      #       autoscale:
      #         min_workers: ${var.job_cluster_min_workers}
      #         max_workers: ${var.job_cluster_max_workers}
      #       autotermination_minutes: 10
      #       custom_tags:
      #         bundle: ${bundle.name}
      #         environment: ${bundle.target}
      #       spark_conf:
      #         "spark.databricks.delta.autoCompact.enabled": "auto"
      #         "spark.databricks.delta.optimizeWrite.enabled": "true"
      #       spark_env_vars:
      #         ENVIRONMENT: ${bundle.target}
      #     # Sample libraries for custom cluster - add at task level instead (see below)

      tasks:  # https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#other-task-settings
        - task_key: ingest_to_raw
          description: Ingest sample data to raw layer

          # For serverless: use environment_key
          environment_key: default

          # For custom cluster: uncomment job_cluster_key and libraries, comment out environment_key
          # job_cluster_key: job_cluster
          # libraries:
          #   - pypi:
          #       package: requests==2.32.3
          #   - pypi:
          #       package: databricks-sql-connector==4.0.0

          max_retries: ${var.max_retries}
          min_retry_interval_millis: ${var.retry_interval_millis}

          spark_python_task:
            python_file: ${workspace.file_path}/src/jobs/ingest_to_raw.py
            parameters:
              - --catalog_name
              - ${var.catalog_name}
              - --environment
              - ${bundle.target}
              - --user_name
              - ${workspace.current_user.short_name}

        # Second task: Transform bronze data to silver layer
        - task_key: transform_to_silver
          description: Transform raw data to silver layer
          depends_on:
            - task_key: ingest_to_raw

          # For serverless: use environment_key
          environment_key: default

          # For custom cluster: uncomment job_cluster_key and libraries, comment out environment_key
          # job_cluster_key: job_cluster
          # libraries:
          #   - pypi:
          #       package: requests==2.32.3

          max_retries: ${var.max_retries}
          min_retry_interval_millis: ${var.retry_interval_millis}

          spark_python_task:
            python_file: ${workspace.file_path}/src/jobs/transform_to_silver.py
            parameters:
              - --catalog_name
              - ${var.catalog_name}
              - --environment
              - ${bundle.target}
              - --user_name
              - ${workspace.current_user.short_name}